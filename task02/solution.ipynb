{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import collections\n",
    "import enum\n",
    "import math\n",
    "import pathlib\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from util import draw_reliability_diagram, cost_function, setup_seeds, calc_calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf `USE_PRETRAINED_INIT` is `True`, then MAP inference uses provided pretrained weights.\\nYou should not modify MAP training or the CNN architecture before passing the hard baseline.\\nIf you set the constant to `False` (to further experiment),\\nthis solution always performs MAP inference before running your SWAG implementation.\\nNote that MAP inference can take a long time.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXTENDED_EVALUATION = False\n",
    "\"\"\"\n",
    "Set `EXTENDED_EVALUATION` to `True` in order to generate additional plots on validation data.\n",
    "\"\"\"\n",
    "\n",
    "USE_PRETRAINED_INIT = True\n",
    "\"\"\"\n",
    "If `USE_PRETRAINED_INIT` is `True`, then MAP inference uses provided pretrained weights.\n",
    "You should not modify MAP training or the CNN architecture before passing the hard baseline.\n",
    "If you set the constant to `False` (to further experiment),\n",
    "this solution always performs MAP inference before running your SWAG implementation.\n",
    "Note that MAP inference can take a long time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceMode(enum.Enum):\n",
    "    \"\"\"\n",
    "    Inference mode switch for your implementation.\n",
    "    `MAP` simply predicts the most likely class using pretrained MAP weights.\n",
    "    `SWAG_DIAGONAL` and `SWAG_FULL` correspond to SWAG-diagonal and the full SWAG method, respectively.\n",
    "    \"\"\"\n",
    "    MAP = 0\n",
    "    SWAG_DIAGONAL = 1\n",
    "    SWAG_FULL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAGInference(object):\n",
    "    \"\"\"\n",
    "    Your implementation of SWA-Gaussian.\n",
    "    This class is used to run and evaluate your solution.\n",
    "    You must preserve all methods and signatures of this class.\n",
    "    However, you can add new methods if you want.\n",
    "\n",
    "    We provide basic functionality and some helper methods.\n",
    "    You can pass all baselines by only modifying methods marked with TODO.\n",
    "    However, we encourage you to skim other methods in order to gain a better understanding of SWAG.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_xs: torch.Tensor,\n",
    "        model_dir: pathlib.Path,\n",
    "        # TODO(1): change inference_mode to InferenceMode.SWAG_DIAGONAL\n",
    "        # TODO(2): change inference_mode to InferenceMode.SWAG_FULL\n",
    "        inference_mode: InferenceMode = InferenceMode.SWAG_FULL,\n",
    "        # TODO(2): optionally add/tweak hyperparameters\n",
    "        swag_epochs: int = 30,\n",
    "        swag_learning_rate: float = 0.045,\n",
    "        swag_update_freq: int = 1,\n",
    "        deviation_matrix_max_rank: int = 15,\n",
    "        bma_samples: int = 30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param train_xs: Training images (for storage only)\n",
    "        :param model_dir: Path to directory containing pretrained MAP weights\n",
    "        :param inference_mode: Control which inference mode (MAP, SWAG-diagonal, full SWAG) to use\n",
    "        :param swag_epochs: Total number of gradient descent epochs for SWAG\n",
    "        :param swag_learning_rate: Learning rate for SWAG gradient descent\n",
    "        :param swag_update_freq: Frequency (in epochs) for updating SWAG statistics during gradient descent\n",
    "        :param deviation_matrix_max_rank: Rank of deviation matrix for full SWAG\n",
    "        :param bma_samples: Number of networks to sample for Bayesian model averaging during prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.inference_mode = inference_mode\n",
    "        self.swag_epochs = swag_epochs\n",
    "        self.swag_learning_rate = swag_learning_rate\n",
    "        self.swag_update_freq = swag_update_freq\n",
    "        self.deviation_matrix_max_rank = deviation_matrix_max_rank\n",
    "        self.bma_samples = bma_samples\n",
    "\n",
    "        # Network used to perform SWAG.\n",
    "        # Note that all operations in this class modify this network IN-PLACE!\n",
    "        self.network = CNN(in_channels=3, out_classes=6)\n",
    "\n",
    "        # Store training dataset to recalculate batch normalization statistics during SWAG inference\n",
    "        self.train_dataset = torch.utils.data.TensorDataset(train_xs)\n",
    "\n",
    "        # SWAG-diagonal\n",
    "        # TODO(1): create attributes for SWAG-diagonal\n",
    "        #  Hint: self._create_weight_copy() creates an all-zero copy of the weights\n",
    "        #  as a dictionary that maps from weight name to values.\n",
    "        #  Hint: you never need to consider the full vector of weights,\n",
    "        #  but can always act on per-layer weights (in the format that _create_weight_copy() returns)\n",
    "        self.weights_list = {name: collections.deque() for name, _ in self.network.named_parameters()}\n",
    "        self.weights_sum = self._create_weight_copy()\n",
    "        self.weights_squares_sum = self._create_weight_copy()\n",
    "        self.weights_first_moment = self._create_weight_copy()\n",
    "        self.weights_second_moment = self._create_weight_copy()\n",
    "        self.weights_first_moment_alt = self._create_weight_copy()\n",
    "        self.weights_second_moment_alt = self._create_weight_copy()\n",
    "        self.weights_num = 0\n",
    "\n",
    "        # Full SWAG\n",
    "        # TODO(2): create attributes for SWAG-diagonal\n",
    "        #  Hint: check collections.deque\n",
    "        self.weights_deviation = {name: collections.deque(maxlen=self.deviation_matrix_max_rank) for name, _ in self.network.named_parameters()}\n",
    "        \n",
    "\n",
    "        # Calibration, prediction, and other attributes\n",
    "        # TODO(2): create additional attributes, e.g., for calibration\n",
    "        self._prediction_threshold = None  # this is an example, feel free to be creative\n",
    "\n",
    "    def update_swag(self) -> None:\n",
    "        \"\"\"\n",
    "        Update SWAG statistics with the current weights of self.network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a copy of the current network weights\n",
    "        current_params = {name: param.detach().clone() for name, param in self.network.named_parameters()}\n",
    "\n",
    "        for name, param in current_params.items():\n",
    "            # SWAG-diagonal\n",
    "            # TODO(1): update SWAG-diagonal attributes for weight `name` using `current_params` and `param`\n",
    "            # Inspired by Welford's online algorithm https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\n",
    "            self.weights_list[name].append(param)\n",
    "            self.weights_sum[name] += param\n",
    "            self.weights_squares_sum[name] += param**2\n",
    "            self.weights_first_moment[name] = ( self.weights_num * self.weights_first_moment[name] + param ) / ( self.weights_num + 1 )\n",
    "            self.weights_second_moment[name] = ( self.weights_num * self.weights_second_moment[name] + param**2 ) / ( self.weights_num + 1 )\n",
    "            self.weights_first_moment_alt[name] += ( param - self.weights_first_moment_alt[name]) / ( self.weights_num + 1 )\n",
    "            self.weights_second_moment_alt[name] += ( param**2 - self.weights_second_moment_alt[name]) / ( self.weights_num + 1 )\n",
    "            \n",
    "            # Full SWAG\n",
    "            if self.inference_mode == InferenceMode.SWAG_FULL:\n",
    "                # TODO(2): update full SWAG attributes for weight `name` using `current_params` and `param`\n",
    "                self.weights_deviation[name].append(param - self.weights_first_moment[name])\n",
    "        \n",
    "        self.weights_num += 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def fit_swag(self, loader: torch.utils.data.DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Fit SWAG on top of the pretrained network self.network.\n",
    "        This method should perform gradient descent with occasional SWAG updates\n",
    "        by calling self.update_swag().\n",
    "        \"\"\"\n",
    "\n",
    "        # We use SGD with momentum and weight decay to perform SWA.\n",
    "        # See the paper on how weight decay corresponds to a type of prior.\n",
    "        # Feel free to play around with optimization hyperparameters.\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.network.parameters(),\n",
    "            lr=self.swag_learning_rate,\n",
    "            momentum=0.9,\n",
    "            nesterov=False,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        loss = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        # TODO(2): Update SWAGScheduler instantiation if you decided to implement a custom schedule.\n",
    "        #  By default, this scheduler just keeps the initial learning rate given to `optimizer`.\n",
    "        lr_scheduler = SWAGScheduler(\n",
    "            optimizer,\n",
    "            epochs=self.swag_epochs,\n",
    "            steps_per_epoch=len(loader),\n",
    "        )\n",
    "\n",
    "        # TODO(1): Perform initialization for SWAG fitting\n",
    "        self.weights_list = {name: collections.deque([param.detach().clone()]) for name, param in self.network.named_parameters()}\n",
    "        self.weights_sum = {name: param.detach().clone() for name, param in self.network.named_parameters()}\n",
    "        self.weights_squares_sum = {name: param.detach().clone()**2 for name, param in self.network.named_parameters()}\n",
    "        self.weights_first_moment = {name: param.detach().clone() for name, param in self.network.named_parameters()}\n",
    "        self.weights_second_moment = {name: param.detach().clone()**2 for name, param in self.network.named_parameters()}\n",
    "        self.weights_first_moment_alt = {name: param.detach().clone() for name, param in self.network.named_parameters()}\n",
    "        self.weights_second_moment_alt = {name: param.detach().clone()**2 for name, param in self.network.named_parameters()}\n",
    "        self.weights_num = 1\n",
    "\n",
    "        self.network.train()\n",
    "        with tqdm.trange(self.swag_epochs, desc=\"Running gradient descent for SWA\") as pbar:\n",
    "            pbar_dict = {}\n",
    "            for epoch in pbar:\n",
    "                average_loss = 0.0\n",
    "                average_accuracy = 0.0\n",
    "                num_samples_processed = 0\n",
    "                for batch_xs, batch_is_snow, batch_is_cloud, batch_ys in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred_ys = self.network(batch_xs)\n",
    "                    batch_loss = loss(input=pred_ys, target=batch_ys)\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    pbar_dict[\"lr\"] = lr_scheduler.get_last_lr()[0]\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "                    # Calculate cumulative average training loss and accuracy\n",
    "                    average_loss = (batch_xs.size(0) * batch_loss.item() + num_samples_processed * average_loss) / (\n",
    "                        num_samples_processed + batch_xs.size(0)\n",
    "                    )\n",
    "                    average_accuracy = (\n",
    "                        torch.sum(pred_ys.argmax(dim=-1) == batch_ys).item()\n",
    "                        + num_samples_processed * average_accuracy\n",
    "                    ) / (num_samples_processed + batch_xs.size(0))\n",
    "                    num_samples_processed += batch_xs.size(0)\n",
    "                    pbar_dict[\"avg. epoch loss\"] = average_loss\n",
    "                    pbar_dict[\"avg. epoch accuracy\"] = average_accuracy\n",
    "                    pbar.set_postfix(pbar_dict)\n",
    "\n",
    "                # TODO(1): Implement periodic SWAG updates using the attributes defined in __init__\n",
    "                if epoch % self.swag_update_freq == 0:\n",
    "                    self.update_swag()\n",
    "\n",
    "    def calibrate(self, validation_data: torch.utils.data.Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Calibrate your predictions using a small validation set.\n",
    "        validation_data contains well-defined and ambiguous samples,\n",
    "        where you can identify the latter by having label -1.\n",
    "        \"\"\"\n",
    "        if self.inference_mode == InferenceMode.MAP:\n",
    "            # In MAP mode, simply predict argmax and do nothing else\n",
    "            self._prediction_threshold = 0.0\n",
    "            return\n",
    "\n",
    "        # TODO(1): pick a prediction threshold, either constant or adaptive.\n",
    "        #  The provided value should suffice to pass the easy baseline.\n",
    "        self._prediction_threshold = 2.0 / 3.0\n",
    "\n",
    "        # TODO(2): perform additional calibration if desired.\n",
    "        #  Feel free to remove or change the prediction threshold.\n",
    "        val_xs, val_is_snow, val_is_cloud, val_ys = validation_data.tensors\n",
    "        assert val_xs.size() == (140, 3, 60, 60)  # N x C x H x W\n",
    "        assert val_ys.size() == (140,)\n",
    "        assert val_is_snow.size() == (140,)\n",
    "        assert val_is_cloud.size() == (140,)\n",
    "\n",
    "    def predict_probabilities_swag(self, loader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform Bayesian model averaging using your SWAG statistics and predict\n",
    "        probabilities for all samples in the loader.\n",
    "        Outputs should be a Nx6 tensor, where N is the number of samples in loader,\n",
    "        and all rows of the output should sum to 1.\n",
    "        That is, output row i column j should be your predicted p(y=j | x_i).\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.eval()\n",
    "\n",
    "        # Perform Bayesian model averaging:\n",
    "        # Instead of sampling self.bma_samples networks (using self.sample_parameters())\n",
    "        # for each datapoint, you can save time by sampling self.bma_samples networks,\n",
    "        # and perform inference with each network on all samples in loader.\n",
    "        per_model_sample_predictions = []\n",
    "        for _ in tqdm.trange(self.bma_samples, desc=\"Performing Bayesian model averaging\"):\n",
    "            # TODO(1): Sample new parameters for self.network from the SWAG approximate posterior\n",
    "            self.sample_parameters()\n",
    "\n",
    "            # TODO(1): Perform inference for all samples in `loader` using current model sample,\n",
    "            #  and add the predictions to per_model_sample_predictions\n",
    "            predictions = self.predict_probabilities_map(loader)\n",
    "            per_model_sample_predictions.append(predictions)\n",
    "\n",
    "        assert len(per_model_sample_predictions) == self.bma_samples\n",
    "        assert all(\n",
    "            isinstance(model_sample_predictions, torch.Tensor)\n",
    "            and model_sample_predictions.dim() == 2  # N x C\n",
    "            and model_sample_predictions.size(1) == 6\n",
    "            for model_sample_predictions in per_model_sample_predictions\n",
    "        )\n",
    "\n",
    "        # TODO(1): Average predictions from different model samples into bma_probabilities\n",
    "        bma_probabilities = torch.stack(per_model_sample_predictions).mean(dim=0)\n",
    "\n",
    "        assert bma_probabilities.dim() == 2 and bma_probabilities.size(1) == 6  # N x C\n",
    "        return bma_probabilities\n",
    "\n",
    "    def sample_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Sample a new network from the approximate SWAG posterior.\n",
    "        For simplicity, this method directly modifies self.network in-place.\n",
    "        Hence, after calling this method, self.network corresponds to a new posterior sample.\n",
    "        \"\"\"\n",
    "\n",
    "        # Instead of acting on a full vector of parameters, all operations can be done on per-layer parameters.\n",
    "        for name, param in self.network.named_parameters():\n",
    "            # SWAG-diagonal part\n",
    "            z_1 = torch.randn(param.size())\n",
    "            # TODO(1): Sample parameter values for SWAG-diagonal\n",
    "            current_mean = self.weights_first_moment[name]\n",
    "            current_std = torch.sqrt(torch.abs(self.weights_second_moment[name] - self.weights_first_moment[name]**2))\n",
    "            \n",
    "            assert current_mean.size() == param.size() and current_std.size() == param.size()\n",
    "\n",
    "            # Diagonal part\n",
    "            sampled_param = current_mean + current_std * z_1\n",
    "\n",
    "            # Full SWAG part\n",
    "            if self.inference_mode == InferenceMode.SWAG_FULL:\n",
    "                # TODO(2): Sample parameter values for full SWAG\n",
    "                kappa = len(self.weights_deviation[name])\n",
    "                z_2 = torch.randn(kappa)\n",
    "                \n",
    "                deviation_matrix = torch.stack(self.weights_deviation[name], axis=-1)\n",
    "                deviance_factor = 1/np.sqrt(2*(kappa-1)) * deviation_matrix\n",
    "                \n",
    "                sampled_param += torch.matmul(deviance_factor,z_2)\n",
    "                sampled_param += (1/np.sqrt(2)-1) * current_std * z_1\n",
    "\n",
    "            # Modify weight value in-place; directly changing self.network\n",
    "            param.data = sampled_param\n",
    "\n",
    "        # TODO(1): Don't forget to update batch normalization statistics using self._update_batchnorm()\n",
    "        #  in the appropriate place!\n",
    "        self._update_batchnorm()\n",
    "\n",
    "    def predict_labels(self, predicted_probabilities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict labels in {0, 1, 2, 3, 4, 5} or \"don't know\" as -1\n",
    "        based on your model's predicted probabilities.\n",
    "        The parameter predicted_probabilities is an Nx6 tensor containing predicted probabilities\n",
    "        as returned by predict_probabilities(...).\n",
    "        The output should be a N-dimensional long tensor, containing values in {-1, 0, 1, 2, 3, 4, 5}.\n",
    "        \"\"\"\n",
    "\n",
    "        # label_probabilities contains the per-row maximum values in predicted_probabilities,\n",
    "        # max_likelihood_labels the corresponding column index (equivalent to class).\n",
    "        label_probabilities, max_likelihood_labels = torch.max(predicted_probabilities, dim=-1)\n",
    "        num_samples, num_classes = predicted_probabilities.size()\n",
    "        assert label_probabilities.size() == (num_samples,) and max_likelihood_labels.size() == (num_samples,)\n",
    "\n",
    "        # A model without uncertainty awareness might simply predict the most likely label per sample:\n",
    "        # return max_likelihood_labels\n",
    "\n",
    "        # A bit better: use a threshold to decide whether to return a label or \"don't know\" (label -1)\n",
    "        # TODO(2): implement a different decision rule if desired\n",
    "        return torch.where(\n",
    "            label_probabilities >= self._prediction_threshold,\n",
    "            max_likelihood_labels,\n",
    "            torch.ones_like(max_likelihood_labels) * -1,\n",
    "        )\n",
    "\n",
    "    def _create_weight_copy(self) -> typing.Dict[str, torch.Tensor]:\n",
    "        \"\"\"Create an all-zero copy of the network weights as a dictionary that maps name -> weight\"\"\"\n",
    "        return {\n",
    "            name: torch.zeros_like(param, requires_grad=False)\n",
    "            for name, param in self.network.named_parameters()\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        loader: torch.utils.data.DataLoader,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform full SWAG fitting procedure.\n",
    "        If `PRETRAINED_WEIGHTS_FILE` is `True`, this method skips the MAP inference part,\n",
    "        and uses pretrained weights instead.\n",
    "\n",
    "        Note that MAP inference can take a very long time.\n",
    "        You should hence only perform MAP inference yourself after passing the hard baseline\n",
    "        using the given CNN architecture and pretrained weights.\n",
    "        \"\"\"\n",
    "\n",
    "        # MAP inference to obtain initial weights\n",
    "        PRETRAINED_WEIGHTS_FILE = self.model_dir / \"map_weights.pt\"\n",
    "        if USE_PRETRAINED_INIT:\n",
    "            self.network.load_state_dict(torch.load(PRETRAINED_WEIGHTS_FILE))\n",
    "            print(\"Loaded pretrained MAP weights from\", PRETRAINED_WEIGHTS_FILE)\n",
    "        else:\n",
    "            self.fit_map(loader)\n",
    "\n",
    "        # SWAG\n",
    "        if self.inference_mode in (InferenceMode.SWAG_DIAGONAL, InferenceMode.SWAG_FULL):\n",
    "            self.fit_swag(loader)\n",
    "\n",
    "    def fit_map(self, loader: torch.utils.data.DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        MAP inference procedure to obtain initial weights of self.network.\n",
    "        This is the exact procedure that was used to obtain the pretrained weights we provide.\n",
    "        \"\"\"\n",
    "        map_epochs = 140\n",
    "        initial_lr = 0.01\n",
    "        decayed_lr = 0.0001\n",
    "        decay_start_epoch = 50\n",
    "        decay_factor = decayed_lr / initial_lr\n",
    "\n",
    "        # Create optimizer, loss, and a learning rate scheduler that aids convergence\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.network.parameters(),\n",
    "            lr=initial_lr,\n",
    "            momentum=0.9,\n",
    "            nesterov=False,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        loss = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            [\n",
    "                torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0),\n",
    "                torch.optim.lr_scheduler.LinearLR(\n",
    "                    optimizer,\n",
    "                    start_factor=1.0,\n",
    "                    end_factor=decay_factor,\n",
    "                    total_iters=(map_epochs - decay_start_epoch) * len(loader),\n",
    "                ),\n",
    "            ],\n",
    "            milestones=[decay_start_epoch * len(loader)],\n",
    "        )\n",
    "\n",
    "        # Put network into training mode\n",
    "        # Batch normalization layers are only updated if the network is in training mode,\n",
    "        # and are replaced by a moving average if the network is in evaluation mode.\n",
    "        self.network.train()\n",
    "        with tqdm.trange(map_epochs, desc=\"Fitting initial MAP weights\") as pbar:\n",
    "            pbar_dict = {}\n",
    "            # Perform the specified number of MAP epochs\n",
    "            for epoch in pbar:\n",
    "                average_loss = 0.0\n",
    "                average_accuracy = 0.0\n",
    "                num_samples_processed = 0\n",
    "                # Iterate over batches of randomly shuffled training data\n",
    "                for batch_xs, _, _, batch_ys in loader:\n",
    "                    # Training step\n",
    "                    optimizer.zero_grad()\n",
    "                    pred_ys = self.network(batch_xs)\n",
    "                    batch_loss = loss(input=pred_ys, target=batch_ys)\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Save learning rate that was used for step, and calculate new one\n",
    "                    pbar_dict[\"lr\"] = lr_scheduler.get_last_lr()[0]\n",
    "                    with warnings.catch_warnings():\n",
    "                        # Suppress annoying warning (that we cannot control) inside PyTorch\n",
    "                        warnings.simplefilter(\"ignore\")\n",
    "                        lr_scheduler.step()\n",
    "\n",
    "                    # Calculate cumulative average training loss and accuracy\n",
    "                    average_loss = (batch_xs.size(0) * batch_loss.item() + num_samples_processed * average_loss) / (\n",
    "                        num_samples_processed + batch_xs.size(0)\n",
    "                    )\n",
    "                    average_accuracy = (\n",
    "                        torch.sum(pred_ys.argmax(dim=-1) == batch_ys).item()\n",
    "                        + num_samples_processed * average_accuracy\n",
    "                    ) / (num_samples_processed + batch_xs.size(0))\n",
    "                    num_samples_processed += batch_xs.size(0)\n",
    "\n",
    "                    pbar_dict[\"avg. epoch loss\"] = average_loss\n",
    "                    pbar_dict[\"avg. epoch accuracy\"] = average_accuracy\n",
    "                    pbar.set_postfix(pbar_dict)\n",
    "\n",
    "    def predict_probabilities(self, xs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the given images xs.\n",
    "        This method returns an NxC float tensor,\n",
    "        where row i column j corresponds to the probability that y_i is class j.\n",
    "\n",
    "        This method uses different strategies depending on self.inference_mode.\n",
    "        \"\"\"\n",
    "        self.network = self.network.eval()\n",
    "\n",
    "        # Create a loader that we can deterministically iterate many times if necessary\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(xs),\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():  # save memory by not tracking gradients\n",
    "            if self.inference_mode == InferenceMode.MAP:\n",
    "                return self.predict_probabilities_map(loader)\n",
    "            else:\n",
    "                return self.predict_probabilities_swag(loader)\n",
    "\n",
    "    def predict_probabilities_map(self, loader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict probabilities assuming that self.network is a MAP estimate.\n",
    "        This simply performs a forward pass for every batch in `loader`,\n",
    "        concatenates all results, and applies a row-wise softmax.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for (batch_xs,) in loader:\n",
    "            predictions.append(self.network(batch_xs))\n",
    "\n",
    "        predictions = torch.cat(predictions)\n",
    "        return torch.softmax(predictions, dim=-1)\n",
    "\n",
    "    def _update_batchnorm(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset and fit batch normalization statistics using the training dataset self.train_dataset.\n",
    "        We provide this method for you for convenience.\n",
    "        See the SWAG paper for why this is required.\n",
    "\n",
    "        Batch normalization usually uses an exponential moving average, controlled by the `momentum` parameter.\n",
    "        However, we are not training but want the statistics for the full training dataset.\n",
    "        Hence, setting `momentum` to `None` tracks a cumulative average instead.\n",
    "        The following code stores original `momentum` values, sets all to `None`,\n",
    "        and restores the previous hyperparameters after updating batchnorm statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        old_momentum_parameters = dict()\n",
    "        for module in self.network.modules():\n",
    "            # Only need to handle batchnorm modules\n",
    "            if not isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
    "                continue\n",
    "\n",
    "            # Store old momentum value before removing it\n",
    "            old_momentum_parameters[module] = module.momentum\n",
    "            module.momentum = None\n",
    "\n",
    "            # Reset batch normalization statistics\n",
    "            module.reset_running_stats()\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        self.network.train()\n",
    "        for (batch_xs,) in loader:\n",
    "            self.network(batch_xs)\n",
    "        self.network.eval()\n",
    "\n",
    "        # Restore old `momentum` hyperparameter values\n",
    "        for module, momentum in old_momentum_parameters.items():\n",
    "            module.momentum = momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAGScheduler(torch.optim.lr_scheduler.LRScheduler):\n",
    "    \"\"\"\n",
    "    Custom learning rate scheduler that calculates a different learning rate each gradient descent step.\n",
    "    The default implementation keeps the original learning rate constant, i.e., does nothing.\n",
    "    You can implement a custom schedule inside calculate_lr,\n",
    "    and add+store additional attributes in __init__.\n",
    "    You should not change any other parts of this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_lr(self, current_epoch: float, old_lr: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the learning rate for the epoch given by current_epoch.\n",
    "        current_epoch is the fractional epoch of SWA fitting, starting at 0.\n",
    "        That is, an integer value x indicates the start of epoch (x+1),\n",
    "        and non-integer values x.y correspond to steps in between epochs (x+1) and (x+2).\n",
    "        old_lr is the previous learning rate.\n",
    "\n",
    "        This method should return a single float: the new learning rate.\n",
    "        \"\"\"\n",
    "        # TODO(2): Implement a custom schedule if desired\n",
    "        return old_lr\n",
    "\n",
    "    # TODO(2): Add and store additional arguments if you decide to implement a custom scheduler\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int,\n",
    "        steps_per_epoch: int,\n",
    "    ):\n",
    "        self.epochs = epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        super().__init__(optimizer, last_epoch=-1, verbose=False)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\n",
    "                \"To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\", UserWarning\n",
    "            )\n",
    "        return [\n",
    "            self.calculate_lr(self.last_epoch / self.steps_per_epoch, group[\"lr\"])\n",
    "            for group in self.optimizer.param_groups\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    swag: SWAGInference,\n",
    "    eval_dataset: torch.utils.data.Dataset,\n",
    "    extended_evaluation: bool,\n",
    "    output_dir: pathlib.Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate your model.\n",
    "    Feel free to change or extend this code.\n",
    "    :param swag: Trained model to evaluate\n",
    "    :param eval_dataset: Validation dataset\n",
    "    :param: extended_evaluation: If True, generates additional plots\n",
    "    :param output_dir: Directory into which extended evaluation plots are saved\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluating model on validation data\")\n",
    "\n",
    "    # We ignore is_snow and is_cloud here, but feel free to use them as well\n",
    "    xs, is_snow, is_cloud, ys = eval_dataset.tensors\n",
    "\n",
    "    # Predict class probabilities on test data,\n",
    "    # most likely classes (according to the max predicted probability),\n",
    "    # and classes as predicted by your SWAG implementation.\n",
    "    pred_prob_all = swag.predict_probabilities(xs)\n",
    "    pred_prob_max, pred_ys_argmax = torch.max(pred_prob_all, dim=-1)\n",
    "    pred_ys = swag.predict_labels(pred_prob_all)\n",
    "\n",
    "    # Create a mask that ignores ambiguous samples (those with class -1)\n",
    "    nonambiguous_mask = ys != -1\n",
    "\n",
    "    # Calculate three kinds of accuracy:\n",
    "    # 1. Overall accuracy, counting \"don't know\" (-1) as its own class\n",
    "    # 2. Accuracy on all samples that have a known label. Predicting -1 on those counts as wrong here.\n",
    "    # 3. Accuracy on all samples that have a known label w.r.t. the class with the highest predicted probability.\n",
    "    accuracy = torch.mean((pred_ys == ys).float()).item()\n",
    "    accuracy_nonambiguous = torch.mean((pred_ys[nonambiguous_mask] == ys[nonambiguous_mask]).float()).item()\n",
    "    accuracy_nonambiguous_argmax = torch.mean(\n",
    "        (pred_ys_argmax[nonambiguous_mask] == ys[nonambiguous_mask]).float()\n",
    "    ).item()\n",
    "    print(f\"Accuracy (raw): {accuracy:.4f}\")\n",
    "    print(f\"Accuracy (non-ambiguous only, your predictions): {accuracy_nonambiguous:.4f}\")\n",
    "    print(f\"Accuracy (non-ambiguous only, predicting most-likely class): {accuracy_nonambiguous_argmax:.4f}\")\n",
    "\n",
    "    # Determine which threshold would yield the smallest cost on the validation data\n",
    "    # Note that this threshold does not necessarily generalize to the test set!\n",
    "    # However, it can help you judge your method's calibration.\n",
    "    thresholds = [0.0] + list(torch.unique(pred_prob_max, sorted=True))\n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        thresholded_ys = torch.where(pred_prob_max <= threshold, -1 * torch.ones_like(pred_ys), pred_ys)\n",
    "        costs.append(cost_function(thresholded_ys, ys).item())\n",
    "    best_idx = np.argmin(costs)\n",
    "    print(f\"Best cost {costs[best_idx]} at threshold {thresholds[best_idx]}\")\n",
    "    print(\"Note that this threshold does not necessarily generalize to the test set!\")\n",
    "\n",
    "    # Calculate ECE and plot the calibration curve\n",
    "    calibration_data = calc_calibration_curve(pred_prob_all.numpy(), ys.numpy(), num_bins=20)\n",
    "    print(\"Validation ECE:\", calibration_data[\"ece\"])\n",
    "\n",
    "    if extended_evaluation:\n",
    "        print(\"Plotting reliability diagram\")\n",
    "        fig = draw_reliability_diagram(calibration_data)\n",
    "        fig.savefig(output_dir / \"reliability_diagram.pdf\")\n",
    "\n",
    "        sorted_confidence_indices = torch.argsort(pred_prob_max)\n",
    "\n",
    "        # Plot samples your model is most confident about\n",
    "        print(\"Plotting most confident validation set predictions\")\n",
    "        most_confident_indices = sorted_confidence_indices[-10:]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(xs[sample_idx].permute(1, 2, 0).numpy())\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f\"pred. {pred_ys[sample_idx]}, true {ys[sample_idx]}\")\n",
    "                bar_colors = [\"C0\"] * 6\n",
    "                if ys[sample_idx] >= 0:\n",
    "                    bar_colors[ys[sample_idx]] = \"C1\"\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(6), pred_prob_all[sample_idx].numpy(), tick_label=np.arange(6), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle(\"Most confident predictions\", size=20)\n",
    "        fig.savefig(output_dir / \"examples_most_confident.pdf\")\n",
    "\n",
    "        # Plot samples your model is least confident about\n",
    "        print(\"Plotting least confident validation set predictions\")\n",
    "        least_confident_indices = sorted_confidence_indices[:10]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(xs[sample_idx].permute(1, 2, 0).numpy())\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f\"pred. {pred_ys[sample_idx]}, true {ys[sample_idx]}\")\n",
    "                bar_colors = [\"C0\"] * 6\n",
    "                if ys[sample_idx] >= 0:\n",
    "                    bar_colors[ys[sample_idx]] = \"C1\"\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(6), pred_prob_all[sample_idx].numpy(), tick_label=np.arange(6), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle(\"Least confident predictions\", size=20)\n",
    "        fig.savefig(output_dir / \"examples_least_confident.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Small convolutional neural network used in this task.\n",
    "    You should not modify this class before passing the hard baseline.\n",
    "\n",
    "    Note that if you change the architecture of this network,\n",
    "    you need to re-run MAP inference and cannot use the provided pretrained weights anymore.\n",
    "    Hence, you need to set `USE_PRETRAINED_INIT = False` at the top of this file.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer0 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=5),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.pool1 = torch.nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.pool2 = torch.nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3),\n",
    "        )\n",
    "\n",
    "        self.global_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.linear = torch.nn.Linear(64, out_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # Average features over both spatial dimensions, and remove the now superfluous dimensions\n",
    "        x = self.global_pool(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "        # Note: this network does NOT output the per-class probabilities y =[y_1, ..., y_C],\n",
    "        # but a feature vector z such that y = softmax(z).\n",
    "        # This avoids numerical instabilities during optimization.\n",
    "        # The PyTorch loss automatically handles this.\n",
    "        log_softmax = self.linear(x)\n",
    "\n",
    "        return log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path.cwd()\n",
    "model_dir = pathlib.Path.cwd()\n",
    "output_dir = pathlib.Path.cwd()\n",
    "\n",
    "# Load training data\n",
    "train_xs = torch.from_numpy(np.load(data_dir / \"train_xs.npz\")[\"train_xs\"])\n",
    "raw_train_meta = np.load(data_dir / \"train_ys.npz\")\n",
    "train_ys = torch.from_numpy(raw_train_meta[\"train_ys\"])\n",
    "train_is_snow = torch.from_numpy(raw_train_meta[\"train_is_snow\"])\n",
    "train_is_cloud = torch.from_numpy(raw_train_meta[\"train_is_cloud\"])\n",
    "dataset_train = torch.utils.data.TensorDataset(train_xs, train_is_snow, train_is_cloud, train_ys)\n",
    "\n",
    "# Load validation data\n",
    "val_xs = torch.from_numpy(np.load(data_dir / \"val_xs.npz\")[\"val_xs\"])\n",
    "raw_val_meta = np.load(data_dir / \"val_ys.npz\")\n",
    "val_ys = torch.from_numpy(raw_val_meta[\"val_ys\"])\n",
    "val_is_snow = torch.from_numpy(raw_val_meta[\"val_is_snow\"])\n",
    "val_is_cloud = torch.from_numpy(raw_val_meta[\"val_is_cloud\"])\n",
    "dataset_val = torch.utils.data.TensorDataset(val_xs, val_is_snow, val_is_cloud, val_ys)\n",
    "\n",
    "# Fix all randomness\n",
    "setup_seeds()\n",
    "\n",
    "# Build and run the actual solution\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "swag = SWAGInference(\n",
    "        train_xs=dataset_train.tensors[0],\n",
    "        model_dir=model_dir,\n",
    "        inference_mode=InferenceMode.SWAG_DIAGONAL,\n",
    "        swag_epochs=30,\n",
    "        bma_samples=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained MAP weights from /Users/ariskoutris/Documents/Probabilistic AI/Projects/PAI-2023/task02/map_weights.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent for SWA:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent for SWA: 100%|██████████| 30/30 [07:31<00:00, 15.03s/it, lr=0.045, avg. epoch loss=0.311, avg. epoch accuracy=0.891]\n"
     ]
    }
   ],
   "source": [
    "swag.fit(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Errors Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the running average of the weight modes introduces some numerical errors. In some cases, this can lead to negative variances. \n",
    "We examine different ways of computing the desired statistics to minimize these numerical errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer0.0.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00002465955913066864 | max: 0.29007959365844726562 | mean: 0.00406648218631744385\n",
      "weights_variance_alt\t\t: min: 0.00002465397119522095 | max: 0.29007768630981445312 | mean: 0.00406648125499486923\n",
      "weights_variance_from_sums\t: min: 0.00002465210855007172 | max: 0.29007959365844726562 | mean: 0.00406648265197873116\n",
      "weights_variance_from_list\t: min: 0.00002547505027905572 | max: 0.29974791407585144043 | mean: 0.00420203199610114098\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer0.0.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00000021452433429658 | max: 0.00001760665327310562 | mean: 0.00000664335811961791\n",
      "weights_variance_alt\t\t: min: 0.00000021445885067806 | max: 0.00001760944724082947 | mean: 0.00000664311210130109\n",
      "weights_variance_from_sums\t: min: 0.00000021451705833897 | max: 0.00001760758459568024 | mean: 0.00000664331128064077\n",
      "weights_variance_from_list\t: min: 0.00000022160337209698 | max: 0.00001819494900701102 | mean: 0.00000686476323608076\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer0.1.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00250698626041412354 | max: 0.12026357650756835938 | mean: 0.01998784020543098450\n",
      "weights_variance_alt\t\t: min: 0.00250700116157531738 | max: 0.12026214599609375000 | mean: 0.01998785324394702911\n",
      "weights_variance_from_sums\t: min: 0.00250701606273651123 | max: 0.12026119232177734375 | mean: 0.01998777315020561218\n",
      "weights_variance_from_list\t: min: 0.00259059411473572254 | max: 0.12427068501710891724 | mean: 0.02065403200685977936\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer0.1.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00128970295190811157 | max: 0.18663716316223144531 | mean: 0.02232504263520240784\n",
      "weights_variance_alt\t\t: min: 0.00128967314958572388 | max: 0.18663716316223144531 | mean: 0.02232500351965427399\n",
      "weights_variance_from_sums\t: min: 0.00128969550132751465 | max: 0.18663716316223144531 | mean: 0.02232502773404121399\n",
      "weights_variance_from_list\t: min: 0.00133269582875072956 | max: 0.19285841286182403564 | mean: 0.02306918799877166748\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer1.0.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00000121921766549349 | max: 0.12890458106994628906 | mean: 0.00103219843003898859\n",
      "weights_variance_alt\t\t: min: 0.00000121939228847623 | max: 0.12890446186065673828 | mean: 0.00103219843003898859\n",
      "weights_variance_from_sums\t: min: 0.00000121927587315440 | max: 0.12890455126762390137 | mean: 0.00103219866286963224\n",
      "weights_variance_from_list\t: min: 0.00000126011320844555 | max: 0.13320142030715942383 | mean: 0.00106660521123558283\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer1.0.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00000000398972588300 | max: 0.00000458047725260258 | mean: 0.00000193428468264756\n",
      "weights_variance_alt\t\t: min: 0.00000000398995325668 | max: 0.00000458024442195892 | mean: 0.00000193424421013333\n",
      "weights_variance_from_sums\t: min: 0.00000000399018063035 | max: 0.00000458024442195892 | mean: 0.00000193427877093200\n",
      "weights_variance_from_list\t: min: 0.00000000412296952135 | max: 0.00000473212048746063 | mean: 0.00000199870009964798\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer1.1.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00128781795501708984 | max: 0.09758186340332031250 | mean: 0.01538480725139379501\n",
      "weights_variance_alt\t\t: min: 0.00128763914108276367 | max: 0.09758257865905761719 | mean: 0.01538474205881357193\n",
      "weights_variance_from_sums\t: min: 0.00128784775733947754 | max: 0.09758281707763671875 | mean: 0.01538484543561935425\n",
      "weights_variance_from_list\t: min: 0.00133057509083300829 | max: 0.10083525627851486206 | mean: 0.01589769124984741211\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer1.1.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00119802448898553848 | max: 0.05198995769023895264 | mean: 0.01198769174516201019\n",
      "weights_variance_alt\t\t: min: 0.00119801890105009079 | max: 0.05198996886610984802 | mean: 0.01198769547045230865\n",
      "weights_variance_from_sums\t: min: 0.00119802448898553848 | max: 0.05198996514081954956 | mean: 0.01198769174516201019\n",
      "weights_variance_from_list\t: min: 0.00123795564286410809 | max: 0.05372297018766403198 | mean: 0.01238728873431682587\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer2.0.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00000386405736207962 | max: 0.10677902400493621826 | mean: 0.00107860402204096317\n",
      "weights_variance_alt\t\t: min: 0.00000386312603950500 | max: 0.10677907615900039673 | mean: 0.00107860402204096317\n",
      "weights_variance_from_sums\t: min: 0.00000386778265237808 | max: 0.10677905380725860596 | mean: 0.00107860413845628500\n",
      "weights_variance_from_list\t: min: 0.00000399227519665146 | max: 0.11033835262060165405 | mean: 0.00111455761361867189\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer2.0.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00000001199077814817 | max: 0.00000484986230731010 | mean: 0.00000141644534323859\n",
      "weights_variance_alt\t\t: min: 0.00000001199396137963 | max: 0.00000484939664602280 | mean: 0.00000141641362461087\n",
      "weights_variance_from_sums\t: min: 0.00000001199123289553 | max: 0.00000485009513795376 | mean: 0.00000141641135087411\n",
      "weights_variance_from_list\t: min: 0.00000001239215130511 | max: 0.00000501191880175611 | mean: 0.00000146364902775531\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer2.1.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00259906053543090820 | max: 0.07501697540283203125 | mean: 0.01317984517663717270\n",
      "weights_variance_alt\t\t: min: 0.00259929895401000977 | max: 0.07501637935638427734 | mean: 0.01317985076457262039\n",
      "weights_variance_from_sums\t: min: 0.00259906053543090820 | max: 0.07501685619354248047 | mean: 0.01317985355854034424\n",
      "weights_variance_from_list\t: min: 0.00268569914624094963 | max: 0.07751724869012832642 | mean: 0.01361919194459915161\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer2.1.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00169822480529546738 | max: 0.06884169578552246094 | mean: 0.01418006047606468201\n",
      "weights_variance_alt\t\t: min: 0.00169822573661804199 | max: 0.06884187459945678711 | mean: 0.01418007537722587585\n",
      "weights_variance_from_sums\t: min: 0.00169822573661804199 | max: 0.06884163618087768555 | mean: 0.01418005116283893585\n",
      "weights_variance_from_list\t: min: 0.00175483268685638905 | max: 0.07113635540008544922 | mean: 0.01465272717177867889\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer3.0.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00000117222225526348 | max: 0.04305078089237213135 | mean: 0.00076237256871536374\n",
      "weights_variance_alt\t\t: min: 0.00000117221134132706 | max: 0.04305081069469451904 | mean: 0.00076237256871536374\n",
      "weights_variance_from_sums\t: min: 0.00000117221134132706 | max: 0.04305081069469451904 | mean: 0.00076237251050770283\n",
      "weights_variance_from_list\t: min: 0.00000121128698538087 | max: 0.04448584094643592834 | mean: 0.00078778498573228717\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer3.0.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00000000510772224516 | max: 0.00000491621904075146 | mean: 0.00000197924009626149\n",
      "weights_variance_alt\t\t: min: 0.00000000510749487148 | max: 0.00000491668470203876 | mean: 0.00000197921463040984\n",
      "weights_variance_from_sums\t: min: 0.00000000510794961883 | max: 0.00000491598621010780 | mean: 0.00000197921599465190\n",
      "weights_variance_from_list\t: min: 0.00000000527793142524 | max: 0.00000507999720866792 | mean: 0.00000204521143132297\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer3.1.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00101673603057861328 | max: 0.04733908176422119141 | mean: 0.00861408095806837082\n",
      "weights_variance_alt\t\t: min: 0.00101649761199951172 | max: 0.04733967781066894531 | mean: 0.00861410796642303467\n",
      "weights_variance_from_sums\t: min: 0.00101673603057861328 | max: 0.04733896255493164062 | mean: 0.00861409492790699005\n",
      "weights_variance_from_list\t: min: 0.00105058774352073669 | max: 0.04891741275787353516 | mean: 0.00890125706791877747\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer3.1.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00157776474952697754 | max: 0.04834292829036712646 | mean: 0.01143451593816280365\n",
      "weights_variance_alt\t\t: min: 0.00157775729894638062 | max: 0.04834291338920593262 | mean: 0.01143452059477567673\n",
      "weights_variance_from_sums\t: min: 0.00157775357365608215 | max: 0.04834289848804473877 | mean: 0.01143451500684022903\n",
      "weights_variance_from_list\t: min: 0.00163035118021070957 | max: 0.04995437338948249817 | mean: 0.01181566435843706131\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer4.0.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00000000055881343997 | max: 0.06319988518953323364 | mean: 0.00052792043425142765\n",
      "weights_variance_alt\t\t: min: 0.00000000055875659655 | max: 0.06319990009069442749 | mean: 0.00052792043425142765\n",
      "weights_variance_from_sums\t: min: 0.00000000055878501826 | max: 0.06319989264011383057 | mean: 0.00052792043425142765\n",
      "weights_variance_from_list\t: min: 0.00000000057739901749 | max: 0.06530653685331344604 | mean: 0.00054551777429878712\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer4.0.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00000000002052402692 | max: 0.00000248395372182131 | mean: 0.00000080203699326375\n",
      "weights_variance_alt\t\t: min: 0.00000000002052225057 | max: 0.00000248383730649948 | mean: 0.00000080203290053760\n",
      "weights_variance_from_sums\t: min: 0.00000000002052225057 | max: 0.00000248430296778679 | mean: 0.00000080203597008222\n",
      "weights_variance_from_list\t: min: 0.00000000002120825043 | max: 0.00000256673911280814 | mean: 0.00000082875703810714\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer4.1.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00328738242387771606 | max: 0.18157982826232910156 | mean: 0.03340659290552139282\n",
      "weights_variance_alt\t\t: min: 0.00328739359974861145 | max: 0.18157982826232910156 | mean: 0.03340662270784378052\n",
      "weights_variance_from_sums\t: min: 0.00328738242387771606 | max: 0.18158054351806640625 | mean: 0.03340659290552139282\n",
      "weights_variance_from_list\t: min: 0.00339696300216019154 | max: 0.18763227760791778564 | mean: 0.03452013060450553894\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer4.1.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00072017312049865723 | max: 0.52208423614501953125 | mean: 0.05119019746780395508\n",
      "weights_variance_alt\t\t: min: 0.00072011351585388184 | max: 0.52208423614501953125 | mean: 0.05119014158844947815\n",
      "weights_variance_from_sums\t: min: 0.00072017312049865723 | max: 0.52208375930786132812 | mean: 0.05119017511606216431\n",
      "weights_variance_from_list\t: min: 0.00074425613274797797 | max: 0.53948622941970825195 | mean: 0.05289650335907936096\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer5.0.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00000000085731244326 | max: 0.00766038149595260620 | mean: 0.00014673155965283513\n",
      "weights_variance_alt\t\t: min: 0.00000000085728402155 | max: 0.00766038894653320312 | mean: 0.00014673155965283513\n",
      "weights_variance_from_sums\t: min: 0.00000000085719875642 | max: 0.00766038149595260620 | mean: 0.00014673155965283513\n",
      "weights_variance_from_list\t: min: 0.00000000088585860869 | max: 0.00791572220623493195 | mean: 0.00015162260388024151\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: layer5.0.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.00032560469117015600 | max: 0.02618628740310668945 | mean: 0.00429857242852449417\n",
      "weights_variance_alt\t\t: min: 0.00032560486579313874 | max: 0.02618628740310668945 | mean: 0.00429856684058904648\n",
      "weights_variance_from_sums\t: min: 0.00032560469117015600 | max: 0.02618622779846191406 | mean: 0.00429856777191162109\n",
      "weights_variance_from_list\t: min: 0.00033645817893557250 | max: 0.02705920673906803131 | mean: 0.00444185547530651093\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: linear.weight\n",
      "\n",
      "weights_variance\t\t: min: 0.00012100487947463989 | max: 0.01224866509437561035 | mean: 0.00282946904189884663\n",
      "weights_variance_alt\t\t: min: 0.00012100953608751297 | max: 0.01224867254495620728 | mean: 0.00282946880906820297\n",
      "weights_variance_from_sums\t: min: 0.00012100394815206528 | max: 0.01224867254495620728 | mean: 0.00282946904189884663\n",
      "weights_variance_from_list\t: min: 0.00012503990728873760 | max: 0.01265695504844188690 | mean: 0.00292378454469144344\n",
      "*****************************************************************************************************************************\n",
      "\n",
      "Layer: linear.bias\n",
      "\n",
      "weights_variance\t\t: min: 0.02157455682754516602 | max: 0.19909691810607910156 | mean: 0.07369418442249298096\n",
      "weights_variance_alt\t\t: min: 0.02157457172870635986 | max: 0.19909667968750000000 | mean: 0.07369424402713775635\n",
      "weights_variance_from_sums\t: min: 0.02157455682754516602 | max: 0.19909691810607910156 | mean: 0.07369416952133178711\n",
      "weights_variance_from_list\t: min: 0.02229370363056659698 | max: 0.20573320984840393066 | mean: 0.07615064829587936401\n",
      "*****************************************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_variance = {}\n",
    "weights_variance_alt = {}\n",
    "weights_variance_from_sums = {}\n",
    "weights_variance_from_list = {}\n",
    "for layer_name, param in swag.network.named_parameters():\n",
    "    weights_variance[layer_name] = swag.weights_second_moment[layer_name].ravel() - swag.weights_first_moment[layer_name].ravel()**2\n",
    "    weights_variance_alt[layer_name] = swag.weights_second_moment_alt[layer_name].ravel() - swag.weights_first_moment_alt[layer_name].ravel()**2\n",
    "    weights_variance_from_sums[layer_name] = swag.weights_squares_sum[layer_name].ravel() / swag.weights_num - swag.weights_sum[layer_name].ravel()**2 / swag.weights_num**2\n",
    "    weights_variance_from_list[layer_name] = torch.stack(list(swag.weights_list[layer_name])).var(dim=0).ravel()\n",
    "\n",
    "for layer_name, param in swag.network.named_parameters():\n",
    "    print(f\"Layer: {layer_name}\\n\")\n",
    "    print(f\"weights_variance\\t\\t: min: {weights_variance[layer_name].min():.20f} | max: {weights_variance[layer_name].max():.20f} | mean: {weights_variance[layer_name].mean():.20f}\")\n",
    "    print(f\"weights_variance_alt\\t\\t: min: {weights_variance_alt[layer_name].min():.20f} | max: {weights_variance_alt[layer_name].max():.20f} | mean: {weights_variance_alt[layer_name].mean():.20f}\")\n",
    "    print(f\"weights_variance_from_sums\\t: min: {weights_variance_from_sums[layer_name].min():.20f} | max: {weights_variance_from_sums[layer_name].max():.20f} | mean: {weights_variance_from_sums[layer_name].mean():.20f}\")\n",
    "    print(f\"weights_variance_from_list\\t: min: {weights_variance_from_list[layer_name].min():.20f} | max: {weights_variance_from_list[layer_name].max():.20f} | mean: {weights_variance_from_list[layer_name].mean():.20f}\")\n",
    "    print(\"*****************************************************************************************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer0.0.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000101858\n",
      "weights_average_alt_error\t: 0.00000000000000076095\n",
      "weights_average_from_sums_error\t: 0.00000000000000062475\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer0.0.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000003402\n",
      "weights_average_alt_error\t: 0.00000000000000003212\n",
      "weights_average_from_sums_error\t: 0.00000000000000004386\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer0.1.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000001532108\n",
      "weights_average_alt_error\t: 0.00000000000001049161\n",
      "weights_average_from_sums_error\t: 0.00000000000000807687\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer0.1.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000234155\n",
      "weights_average_alt_error\t: 0.00000000000000324751\n",
      "weights_average_from_sums_error\t: 0.00000000000000071872\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer1.0.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000015976\n",
      "weights_average_alt_error\t: 0.00000000000000011767\n",
      "weights_average_from_sums_error\t: 0.00000000000000010534\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer1.0.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000001017\n",
      "weights_average_alt_error\t: 0.00000000000000001422\n",
      "weights_average_from_sums_error\t: 0.00000000000000000790\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer1.1.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000001998401\n",
      "weights_average_alt_error\t: 0.00000000000001199041\n",
      "weights_average_from_sums_error\t: 0.00000000000001443290\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer1.1.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000155746\n",
      "weights_average_alt_error\t: 0.00000000000000058037\n",
      "weights_average_from_sums_error\t: 0.00000000000000123003\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer2.0.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000015985\n",
      "weights_average_alt_error\t: 0.00000000000000011668\n",
      "weights_average_from_sums_error\t: 0.00000000000000009932\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer2.0.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000001687\n",
      "weights_average_alt_error\t: 0.00000000000000000786\n",
      "weights_average_from_sums_error\t: 0.00000000000000000762\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer2.1.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000777156\n",
      "weights_average_alt_error\t: 0.00000000000000754952\n",
      "weights_average_from_sums_error\t: 0.00000000000000666134\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer2.1.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000224017\n",
      "weights_average_alt_error\t: 0.00000000000000223912\n",
      "weights_average_from_sums_error\t: 0.00000000000000085398\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer3.0.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000011001\n",
      "weights_average_alt_error\t: 0.00000000000000008636\n",
      "weights_average_from_sums_error\t: 0.00000000000000007165\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer3.0.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000001201\n",
      "weights_average_alt_error\t: 0.00000000000000001315\n",
      "weights_average_from_sums_error\t: 0.00000000000000001036\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer3.1.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000001559863\n",
      "weights_average_alt_error\t: 0.00000000000001521006\n",
      "weights_average_from_sums_error\t: 0.00000000000000915934\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer3.1.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000261445\n",
      "weights_average_alt_error\t: 0.00000000000000227097\n",
      "weights_average_from_sums_error\t: 0.00000000000000190755\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer4.0.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000007704\n",
      "weights_average_alt_error\t: 0.00000000000000005854\n",
      "weights_average_from_sums_error\t: 0.00000000000000004965\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer4.0.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000000528\n",
      "weights_average_alt_error\t: 0.00000000000000000520\n",
      "weights_average_from_sums_error\t: 0.00000000000000000259\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer4.1.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000449045\n",
      "weights_average_alt_error\t: 0.00000000000000475168\n",
      "weights_average_from_sums_error\t: 0.00000000000000357100\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer4.1.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000001423861\n",
      "weights_average_alt_error\t: 0.00000000000001232348\n",
      "weights_average_from_sums_error\t: 0.00000000000000734135\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer5.0.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000002836\n",
      "weights_average_alt_error\t: 0.00000000000000002211\n",
      "weights_average_from_sums_error\t: 0.00000000000000001817\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: layer5.0.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000077918\n",
      "weights_average_alt_error\t: 0.00000000000000115847\n",
      "weights_average_from_sums_error\t: 0.00000000000000083944\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: linear.weight\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000000038332\n",
      "weights_average_alt_error\t: 0.00000000000000032730\n",
      "weights_average_from_sums_error\t: 0.00000000000000025115\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n",
      "Layer: linear.bias\n",
      "\n",
      "weights_average_error\t\t: 0.00000000000001065814\n",
      "weights_average_alt_error\t: 0.00000000000000310862\n",
      "weights_average_from_sums_error\t: 0.00000000000000828966\n",
      "weights_average_from_list_error\t: 0.00000000000000000000\n",
      "*************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_average_error = {}\n",
    "weights_average_alt_error = {}\n",
    "weights_average_from_sums_error = {}\n",
    "weights_average_from_list_error = {}\n",
    "for layer_name, param in swag.network.named_parameters():\n",
    "    weights_true_average = torch.stack(list(swag.weights_list[layer_name])).mean(dim=0)\n",
    "    weights_average_error[layer_name] = swag.weights_first_moment[layer_name] - weights_true_average\n",
    "    weights_average_alt_error[layer_name] = swag.weights_first_moment_alt[layer_name] - weights_true_average\n",
    "    weights_average_from_sums_error[layer_name] = (swag.weights_sum[layer_name] / swag.weights_num) - weights_true_average\n",
    "    weights_average_from_list_error[layer_name] = torch.stack(list(swag.weights_list[layer_name])).mean(dim=0) - weights_true_average\n",
    "\n",
    "    weights_average_error[layer_name] = torch.mean(weights_average_error[layer_name]**2)\n",
    "    weights_average_alt_error[layer_name] = torch.mean(weights_average_alt_error[layer_name]**2)\n",
    "    weights_average_from_sums_error[layer_name] = torch.mean(weights_average_from_sums_error[layer_name]**2)\n",
    "    weights_average_from_list_error[layer_name] = torch.mean(weights_average_from_list_error[layer_name]**2)\n",
    "\n",
    "for layer_name, param in swag.network.named_parameters():\n",
    "    print(f\"Layer: {layer_name}\\n\")\n",
    "    print(f\"weights_average_error\\t\\t: {weights_average_error[layer_name]:.20f}\")\n",
    "    print(f\"weights_average_alt_error\\t: {weights_average_alt_error[layer_name]:.20f}\")\n",
    "    print(f\"weights_average_from_sums_error\\t: {weights_average_from_sums_error[layer_name]:.20f}\")\n",
    "    print(f\"weights_average_from_list_error\\t: {weights_average_from_list_error[layer_name]:.20f}\")\n",
    "    print(\"*************************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_average_error_median\t\t: 0.00000000000000089888\n",
      "weights_average_alt_error_median\t: 0.00000000000000067066\n",
      "weights_average_from_sums_error_median\t: 0.00000000000000067173\n",
      "weights_average_from_list_error_median\t: 0.00000000000000000000\n"
     ]
    }
   ],
   "source": [
    "weights_average_error_median = np.median(list(weights_average_error.values()))\n",
    "weights_average_alt_error_median = np.median(list(weights_average_alt_error.values()))\n",
    "weights_average_from_sums_error_median = np.median(list(weights_average_from_sums_error.values()))\n",
    "weights_average_from_list_error_median = np.median(list(weights_average_from_list_error.values()))\n",
    "\n",
    "print(f\"weights_average_error_median\\t\\t: {weights_average_error_median:.20f}\")\n",
    "print(f\"weights_average_alt_error_median\\t: {weights_average_alt_error_median:.20f}\")\n",
    "print(f\"weights_average_from_sums_error_median\\t: {weights_average_from_sums_error_median:.20f}\")\n",
    "print(f\"weights_average_from_list_error_median\\t: {weights_average_from_list_error_median:.20f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "plt.hist(swag.weights_first_moment[layer_name].ravel(), bins=30, alpha=0.5, label='First Moment')\n",
    "plt.hist(swag.weights_second_moment[layer_name].ravel(), bins=30, alpha=0.5, label='Second Moment')\n",
    "\n",
    "plt.title(f'Weight Modes ({layer_name})')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(weights_variance[layer_name], bins=30, alpha=0.5, label='Variance')\n",
    "\n",
    "plt.title(f'Weight Variance ({layer_name})')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.hist(weights_avg_errors.detach().numpy().ravel(), bins=30, alpha=0.5, label='Incremental - Cumulative Average')\n",
    "plt.title(\"Incremental - Cumulative Average\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
